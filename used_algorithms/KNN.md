# K-Nearest Neighbors (KNN)

DziaÅ‚a na zasadzie podobieÅ„stwa miÄ™dzy prÃ³bkami. W przypadku nowej prÃ³bki, algorytm szuka ğ‘˜ najbliÅ¼szych sÄ…siadÃ³w w przestrzeni cech i przypisuje klasÄ™ na podstawie wiÄ™kszoÅ›ci etykiet sÄ…siadÃ³w.

## Jak dziaÅ‚a KNN?

1. Algorytm oblicza odlegÅ‚oÅ›ci miÄ™dzy nowÄ… prÃ³bkÄ… a wszystkimi prÃ³bkami w zbiorze treningowym (np. za pomocÄ… odlegÅ‚oÅ›ci euklidesowej).

2. Wybiera ğ‘˜ najbliÅ¼szych sÄ…siadÃ³w (prÃ³bek z najmniejszÄ… odlegÅ‚oÅ›ciÄ…).
3. Klasyfikuje nowÄ… prÃ³bkÄ™ jako tÄ™ klasÄ™, ktÃ³ra wystÄ™puje najczÄ™Å›ciej wÅ›rÃ³d tych sÄ…siadÃ³w (gÅ‚osowanie wiÄ™kszoÅ›ciowe).

Parametr ğ‘˜ (liczba sÄ…siadÃ³w) to kluczowy hiperparametr, ktÃ³ry wpÅ‚ywa na dokÅ‚adnoÅ›Ä‡ modelu.

## Zalety i wady

Zalety:

- Prosty do zrozumienia i zaimplementowania.
- Nie wymaga "uczenia" â€“ wystarczy zapisaÄ‡ dane treningowe.
- MoÅ¼e dziaÅ‚aÄ‡ dobrze na maÅ‚ych zbiorach danych.

Wady:

- Wolny dla duÅ¼ych zbiorÃ³w danych (duÅ¼o obliczeÅ„ w czasie predykcji).
- WraÅ¼liwy na szum w danych (dobÃ³r ğ‘˜ ma kluczowe znaczenie).
- Nie dziaÅ‚a dobrze na danych o duÅ¼ej liczbie wymiarÃ³w (tzw. curse of dimensionality).